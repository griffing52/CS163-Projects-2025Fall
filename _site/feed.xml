<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/CS163-Projects-2025Fall/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/CS163-Projects-2025Fall/" rel="alternate" type="text/html" /><updated>2025-12-13T17:25:03-08:00</updated><id>http://localhost:4000/CS163-Projects-2025Fall/feed.xml</id><title type="html">2025F, UCLA CS163 Course Projects</title><subtitle>Course projects for UCLA CS163, Deep Learning in Compuver Vision</subtitle><author><name>UCLAdeepvision</name></author><entry><title type="html">Diagnosing and Improving Boundary Failures in Urban Semantic Segmentation via SAM3-Guided Supervision</title><link href="http://localhost:4000/CS163-Projects-2025Fall/2025/03/01/team43.html" rel="alternate" type="text/html" title="Diagnosing and Improving Boundary Failures in Urban Semantic Segmentation via SAM3-Guided Supervision" /><published>2025-03-01T00:00:00-08:00</published><updated>2025-03-01T00:00:00-08:00</updated><id>http://localhost:4000/CS163-Projects-2025Fall/2025/03/01/team43</id><content type="html" xml:base="http://localhost:4000/CS163-Projects-2025Fall/2025/03/01/team43.html"><![CDATA[<blockquote>
  <p>Semantic segmentation models achieve high overall accuracy on urban datasets, yet systematically fail on thin structures and object boundaries critical for safety-relevant perception.</p>

  <p>This project presents a diagnostic-driven framework for understanding segmentation failures on Cityscapes and introduces a SAM3-guided boundary supervision method that injects geometric priors into SegFormer. By combining cross-model difficulty analysis with geometry-aware auxiliary training, we demonstrate targeted improvements on thin and boundary-sensitive classes without increasing inference cost.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#1-introduction" id="markdown-toc-1-introduction">1. Introduction</a></li>
  <li><a href="#2-background-and-related-work" id="markdown-toc-2-background-and-related-work">2. Background and Related Work</a>    <ul>
      <li><a href="#21-semantic-segmentation-in-urban-scenes" id="markdown-toc-21-semantic-segmentation-in-urban-scenes">2.1 Semantic Segmentation in Urban Scenes</a></li>
      <li><a href="#22-mask-based-and-universal-segmentation-models" id="markdown-toc-22-mask-based-and-universal-segmentation-models">2.2 Mask-Based and Universal Segmentation Models</a></li>
      <li><a href="#23-foundation-models-and-geometric-priors" id="markdown-toc-23-foundation-models-and-geometric-priors">2.3 Foundation Models and Geometric Priors</a></li>
    </ul>
  </li>
  <li><a href="#3-cross-model-difficulty-analysis-framework" id="markdown-toc-3-cross-model-difficulty-analysis-framework">3. Cross-Model Difficulty Analysis Framework</a>    <ul>
      <li><a href="#31-motivation" id="markdown-toc-31-motivation">3.1 Motivation</a></li>
      <li><a href="#32-difficulty-clustering" id="markdown-toc-32-difficulty-clustering">3.2 Difficulty Clustering</a></li>
      <li><a href="#33-attribute-and-pattern-analysis" id="markdown-toc-33-attribute-and-pattern-analysis">3.3 Attribute and Pattern Analysis</a></li>
    </ul>
  </li>
  <li><a href="#4-method-overview-geometry-aware-supervision" id="markdown-toc-4-method-overview-geometry-aware-supervision">4. Method Overview: Geometry-Aware Supervision</a>    <ul>
      <li><a href="#41-design-rationale" id="markdown-toc-41-design-rationale">4.1 Design Rationale</a></li>
      <li><a href="#42-why-sam3" id="markdown-toc-42-why-sam3">4.2 Why SAM3?</a></li>
      <li><a href="#43-key-idea" id="markdown-toc-43-key-idea">4.3 Key Idea</a></li>
    </ul>
  </li>
  <li><a href="#5-model-architecture" id="markdown-toc-5-model-architecture">5. Model Architecture</a>    <ul>
      <li><a href="#51-high-level-architecture" id="markdown-toc-51-high-level-architecture">5.1 High-Level Architecture</a></li>
      <li><a href="#52-boundary-head" id="markdown-toc-52-boundary-head">5.2 Boundary Head</a></li>
    </ul>
  </li>
  <li><a href="#6-implementation-details" id="markdown-toc-6-implementation-details">6. Implementation Details</a>    <ul>
      <li><a href="#61-framework-and-tools" id="markdown-toc-61-framework-and-tools">6.1 Framework and Tools</a></li>
      <li><a href="#62-data-preparation" id="markdown-toc-62-data-preparation">6.2 Data Preparation</a></li>
      <li><a href="#63-training-strategies" id="markdown-toc-63-training-strategies">6.3 Training Strategies</a></li>
    </ul>
  </li>
  <li><a href="#7-experimental-setup" id="markdown-toc-7-experimental-setup">7. Experimental Setup</a>    <ul>
      <li><a href="#71-dataset" id="markdown-toc-71-dataset">7.1 Dataset</a></li>
      <li><a href="#72-models-evaluated" id="markdown-toc-72-models-evaluated">7.2 Models Evaluated</a></li>
      <li><a href="#73-metrics" id="markdown-toc-73-metrics">7.3 Metrics</a></li>
    </ul>
  </li>
  <li><a href="#8-results" id="markdown-toc-8-results">8. Results</a>    <ul>
      <li><a href="#81-quantitative-results" id="markdown-toc-81-quantitative-results">8.1 Quantitative Results</a></li>
      <li><a href="#overall-performance-comparison" id="markdown-toc-overall-performance-comparison">Overall Performance Comparison</a></li>
      <li><a href="#impact-of-sam3-boundary-supervision-on-thin-classes" id="markdown-toc-impact-of-sam3-boundary-supervision-on-thin-classes">Impact of SAM3 Boundary Supervision on Thin Classes</a></li>
      <li><a href="#cross-model-comparison-on-thin-structures" id="markdown-toc-cross-model-comparison-on-thin-structures">Cross-Model Comparison on Thin Structures</a></li>
      <li><a href="#82-qualitative-results" id="markdown-toc-82-qualitative-results">8.2 Qualitative Results</a></li>
    </ul>
  </li>
  <li><a href="#9-analysis-and-observations" id="markdown-toc-9-analysis-and-observations">9. Analysis and Observations</a></li>
  <li><a href="#10-limitations" id="markdown-toc-10-limitations">10. Limitations</a></li>
  <li><a href="#11-conclusion" id="markdown-toc-11-conclusion">11. Conclusion</a></li>
  <li><a href="#12-future-work" id="markdown-toc-12-future-work">12. Future Work</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/43/SAM3_output.png" alt="Difference" style="width: 1000px; max-width: 100%;" />
  <p><em>
    Fig 1. SAM3 output and example of the semantic segmentation problem in urban scenes.
  </em></p>
</div>

<p>Semantic segmentation of urban street scenes is a foundational task for autonomous driving, robotics, and urban mapping. Modern deep learning models—particularly transformer-based architectures—achieve strong performance on large, visually dominant classes such as roads, buildings, and sky (Example shown in <em>Figure 1.</em>). However, despite high mean Intersection-over-Union (mIoU), these models exhibit consistent failure modes when segmenting <strong>thin structures</strong> (e.g., poles, traffic signs, bicycle frames) and <strong>complex object boundaries</strong>.</p>

<p>These failure modes are problematic for two reasons. First, thin objects often correspond to safety-critical infrastructure. Second, boundary errors frequently propagate to downstream tasks such as tracking and planning. Yet, aggregate metrics like mIoU obscure <em>why</em> these failures occur and <em>which</em> images or structures are most affected.</p>

<p>This project takes a <strong>diagnostic-first approach</strong>. Rather than focusing solely on improving global performance, we first analyze segmentation difficulty across models and images to identify systematic weaknesses. Motivated by this analysis, we propose a <strong>SAM3-guided boundary supervision strategy</strong> that distills geometric priors from a large foundation model into a lightweight semantic segmentation network.</p>

<h2 id="2-background-and-related-work">2. Background and Related Work</h2>

<h3 id="21-semantic-segmentation-in-urban-scenes">2.1 Semantic Segmentation in Urban Scenes</h3>

<p>Early semantic segmentation approaches relied on hand-crafted features and graphical models such as Conditional Random Fields. While effective at modeling local boundaries, these methods struggled with semantic consistency at scale.</p>

<p>The introduction of fully convolutional networks (FCNs) enabled end-to-end semantic segmentation, later improved by architectures such as DeepLab through atrous convolutions and multi-scale context. More recently, transformer-based models such as SegFormer have demonstrated strong performance by modeling long-range dependencies with efficient attention mechanisms.</p>

<p>Despite these advances, <strong>boundary precision remains a known weakness</strong>, particularly for thin or elongated structures.</p>

<h3 id="22-mask-based-and-universal-segmentation-models">2.2 Mask-Based and Universal Segmentation Models</h3>

<p>Mask-based architectures, such as Mask2Former, unify semantic, instance, and panoptic segmentation through mask classification. These methods improve object delineation but are computationally heavier and less suitable for lightweight deployment.</p>

<h3 id="23-foundation-models-and-geometric-priors">2.3 Foundation Models and Geometric Priors</h3>

<p>Foundation models such as the Segment Anything Model (SAM) and its successor SAM3 are trained on large-scale data to produce high-quality object masks with strong geometric fidelity. However, these models are class-agnostic and are not directly optimized for semantic segmentation benchmarks.</p>

<p>Recent work suggests that such models can act as <strong>teachers</strong>, providing structural priors that complement task-specific learners.</p>

<h2 id="3-cross-model-difficulty-analysis-framework">3. Cross-Model Difficulty Analysis Framework</h2>

<h3 id="31-motivation">3.1 Motivation</h3>

<p>Standard evaluation metrics fail to explain <em>where</em> and <em>why</em> segmentation models fail. To address this limitation, we introduce a framework for analyzing segmentation difficulty at the <strong>image level</strong> across multiple architectures.</p>

<h3 id="32-difficulty-clustering">3.2 Difficulty Clustering</h3>

<p>For each image in the validation set, we compute a performance vector containing per-class IoU scores from multiple models. We then apply clustering methods (e.g., k-means or hierarchical clustering) to group images by shared difficulty patterns.</p>

<p>The resulting clusters are categorized as:</p>
<ul>
  <li><strong>Universally Easy</strong>: all models perform well</li>
  <li><strong>Universally Hard</strong>: all models fail</li>
  <li><strong>Model-Specific</strong>: performance varies by architecture</li>
  <li><strong>Ambiguous</strong>: inconsistent or unstable behavior</li>
</ul>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/43/SAM3_vs_SegFormer.png" alt="Difference" style="width: 800px; max-width: 100%;" />
  <p><em>
    Fig 1. Venn Diagram Analysis of Model Consensus on Thin Object Classes.
    Comparison of successful predictions (IoU &gt; 0.6) and failure cases (IoU &lt; 0.5)
    across SAM3-solo, SegFormer (Baseline), and our proposed SegFormer+SAM3 method.
    (Left) Shared Success: Our method (blue) not only retains the majority of
    baseline successes (102 shared instances) but also uniquely correctly segments
    59 difficult instances that both the standalone SegFormer and SAM3 failed to
    capture. (Right) Shared Failure: The proposed method demonstrates superior
    robustness, having the fewest unique failures (9) compared to SAM3-solo (62)
    and SegFormer (22), indicating that the boundary supervision effectively
    corrects errors inherent to the individual models.
  </em></p>
</div>

<h3 id="33-attribute-and-pattern-analysis">3.3 Attribute and Pattern Analysis</h3>

<p>We analyze correlations between difficulty clusters and image attributes such as object size, occlusion, clutter, and boundary density. This analysis reveals that thin structures and boundary-heavy scenes are overrepresented in hard clusters, particularly for transformer-based models.</p>

<h2 id="4-method-overview-geometry-aware-supervision">4. Method Overview: Geometry-Aware Supervision</h2>

<h3 id="41-design-rationale">4.1 Design Rationale</h3>

<p>Our analysis indicates that many segmentation failures arise not from semantic confusion, but from <strong>poor geometric localization</strong>. We therefore seek to inject geometric priors into a standard semantic segmentation model without altering its inference-time complexity.</p>

<h3 id="42-why-sam3">4.2 Why SAM3?</h3>

<p>SAM3 provides high-quality, class-agnostic object boundaries learned from large-scale data. Rather than using SAM3 directly for semantic segmentation, we employ it as a <strong>frozen geometry teacher</strong> during training.</p>

<h3 id="43-key-idea">4.3 Key Idea</h3>

<p>We augment SegFormer with an auxiliary boundary prediction task. During training, the model learns to:</p>
<ol>
  <li>Predict semantic labels (standard objective)</li>
  <li>Predict object boundaries aligned with SAM3 outputs (auxiliary objective)</li>
</ol>

<p>Importantly, this auxiliary loss is applied <strong>only to thin and boundary-sensitive classes</strong>, preventing over-regularization of large regions.</p>

<h2 id="5-model-architecture">5. Model Architecture</h2>

<h3 id="51-high-level-architecture">5.1 High-Level Architecture</h3>

<p>The proposed system consists of:</p>
<ul>
  <li>A SegFormer backbone (MiT-B1)</li>
  <li>A standard semantic segmentation head</li>
  <li>An auxiliary boundary prediction head</li>
  <li>A frozen SAM3 model used to generate boundary targets</li>
</ul>

<p>At inference time, only the SegFormer model is used.</p>

<!-- Figure suggestion -->
<!-- Fig. 2: High-level architecture diagram showing SegFormer backbone, boundary head, and SAM3 teacher -->

<h3 id="52-boundary-head">5.2 Boundary Head</h3>

<p>The boundary head is a lightweight convolutional module attached to intermediate feature maps. It predicts a boundary probability map supervised by edges extracted from SAM3 masks.</p>

<p>The total training loss is:</p>

\[\mathcal{L} = \mathcal{L}_{seg} + \lambda \mathcal{L}_{boundary}\]

<p>where \(\mathcal{L}_{boundary}\) is applied only on pixels belonging to thin classes.</p>

<div style="text-align: center;">
  <img src="/CS163-Projects-2025Fall/assets/images/43/thin_class.png" alt="SAM3" style="width: 800px; max-width: 100%;" />
  <p><em>Fig 2. Example boundary target generated by SAM3.</em></p>
</div>

<h2 id="6-implementation-details">6. Implementation Details</h2>

<h3 id="61-framework-and-tools">6.1 Framework and Tools</h3>

<ul>
  <li>PyTorch</li>
  <li>HuggingFace Transformers</li>
  <li>Cityscapes dataset API</li>
</ul>

<h3 id="62-data-preparation">6.2 Data Preparation</h3>

<ul>
  <li>Cityscapes class mapping</li>
  <li>Thin-class mask generation</li>
  <li>SAM3 boundary extraction and binarization</li>
</ul>

<h3 id="63-training-strategies">6.3 Training Strategies</h3>

<ul>
  <li>Baseline SegFormer training</li>
  <li>Online Hard Example Mining (OHEM)</li>
  <li>Oversampling of underrepresented classes</li>
  <li>SAM3-guided auxiliary supervision</li>
</ul>

<h2 id="7-experimental-setup">7. Experimental Setup</h2>

<h3 id="71-dataset">7.1 Dataset</h3>

<p>Experiments are conducted on the Cityscapes dataset using the official train/validation split.</p>

<h3 id="72-models-evaluated">7.2 Models Evaluated</h3>

<ul>
  <li>SegFormer (B1)</li>
  <li>SegFormer + OHEM</li>
  <li>Mask2Former</li>
  <li>DDRNet-23 Slim</li>
  <li>SAM3 (zero-shot evaluation)</li>
  <li>SAM3-Guided SegFormer (proposed)</li>
</ul>

<h3 id="73-metrics">7.3 Metrics</h3>

<ul>
  <li>Per-class IoU</li>
  <li>Mean IoU</li>
  <li>Thin-class focused analysis</li>
</ul>

<h2 id="8-results">8. Results</h2>

<h3 id="81-quantitative-results">8.1 Quantitative Results</h3>

<p>We evaluate all models on the Cityscapes validation set using mean Intersection-over-Union (mIoU) and per-class IoU. Results are reported for standard semantic segmentation models, zero-shot SAM3, and our proposed SAM3-guided SegFormer variants.</p>

<h3 id="overall-performance-comparison">Overall Performance Comparison</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: center">mIoU ↑</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>DDRNet-23 Slim</strong></td>
      <td style="text-align: center"><strong>76.83</strong></td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>SegFormer + OHEM (Ours)</strong></td>
      <td style="text-align: center"><strong>73.45</strong></td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>SegFormer + SAM3 Boundary (Ours)</strong></td>
      <td style="text-align: center"><strong>73.32</strong></td>
    </tr>
    <tr>
      <td style="text-align: left">SegFormer</td>
      <td style="text-align: center">70.95</td>
    </tr>
    <tr>
      <td style="text-align: left">Mask2Former</td>
      <td style="text-align: center">64.53</td>
    </tr>
    <tr>
      <td style="text-align: left">SAM3 (Zero-shot)</td>
      <td style="text-align: center">62.14</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 1.</strong> <em>Mean IoU comparison across evaluated models on the Cityscapes validation set.</em></p>

<p><strong>Analysis.</strong><br />
DDRNet-23 Slim achieves the highest overall mIoU (76.83%), reflecting its strong inductive bias toward preserving fine spatial detail through a dual-resolution architecture. However, this performance comes at the cost of architectural specialization and reduced flexibility compared to transformer-based models.</p>

<p>Among transformer-based approaches, SegFormer serves as a strong baseline but lags behind both of our finetuned variants. SegFormer with Online Hard Example Mining (OHEM) achieves the highest mIoU among SegFormer variants (73.45%), indicating that emphasizing difficult samples improves global robustness. The proposed SAM3-guided SegFormer achieves a comparable mIoU (73.32%) while specifically targeting boundary-related failure modes, as discussed next.</p>

<p>Mask2Former and zero-shot SAM3 trail behind due to, respectively, heavy architectural overhead and lack of semantic supervision.</p>

<h3 id="impact-of-sam3-boundary-supervision-on-thin-classes">Impact of SAM3 Boundary Supervision on Thin Classes</h3>

<p>To isolate the effect of geometry-aware supervision, we compare SegFormer against its SAM3-guided counterpart on thin and boundary-sensitive classes.</p>

<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>SegFormer</th>
      <th>SAM3-Boundary</th>
      <th style="text-align: right">Δ IoU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Pole</strong></td>
      <td>51.49</td>
      <td><strong>56.06</strong></td>
      <td style="text-align: right"><strong>+4.57</strong></td>
    </tr>
    <tr>
      <td><strong>Fence</strong></td>
      <td>51.84</td>
      <td><strong>55.73</strong></td>
      <td style="text-align: right"><strong>+3.89</strong></td>
    </tr>
    <tr>
      <td><strong>Traffic Light</strong></td>
      <td>61.06</td>
      <td><strong>66.53</strong></td>
      <td style="text-align: right"><strong>+5.47</strong></td>
    </tr>
    <tr>
      <td><strong>Traffic Sign</strong></td>
      <td>71.66</td>
      <td><strong>75.37</strong></td>
      <td style="text-align: right"><strong>+3.71</strong></td>
    </tr>
    <tr>
      <td><strong>Rider</strong></td>
      <td>37.35</td>
      <td><strong>52.51</strong></td>
      <td style="text-align: right"><strong>+15.16</strong></td>
    </tr>
    <tr>
      <td><strong>Motorcycle</strong></td>
      <td>47.29</td>
      <td><strong>61.22</strong></td>
      <td style="text-align: right"><strong>+13.93</strong></td>
    </tr>
    <tr>
      <td><strong>Bicycle</strong></td>
      <td>69.62</td>
      <td><strong>73.46</strong></td>
      <td style="text-align: right"><strong>+3.84</strong></td>
    </tr>
  </tbody>
</table>

<p><strong>Table 2.</strong> <em>Per-class IoU improvements obtained by SAM3-guided boundary supervision on thin and boundary-sensitive categories.</em></p>

<p><strong>Analysis</strong><br />
The proposed SAM3-guided SegFormer improves mean IoU by <strong>+2.37</strong> over the baseline SegFormer, with gains concentrated almost exclusively on thin and boundary-dominated classes. The most pronounced improvements are observed for <em>rider</em> (+15.16) and <em>motorcycle</em> (+13.93), classes characterized by overlapping instances and ambiguous boundaries.</p>

<p>These results strongly support the central hypothesis of this project: <strong>injecting geometric priors via boundary supervision disproportionately benefits thin and overlapping objects</strong>, which are poorly served by region-based losses alone.</p>

<h3 id="cross-model-comparison-on-thin-structures">Cross-Model Comparison on Thin Structures</h3>

<p>We further compare performance on thin classes across all evaluated models to contextualize the improvements.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Class</th>
      <th style="text-align: right">SegFormer</th>
      <th style="text-align: right">SegFormer OHEM</th>
      <th style="text-align: right">SegFormer + SAM3</th>
      <th style="text-align: right">DDRNet-23</th>
      <th style="text-align: right">Mask2Former</th>
      <th style="text-align: right">SAM3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Wall</td>
      <td style="text-align: right">51.08</td>
      <td style="text-align: right">54.12</td>
      <td style="text-align: right"><strong>55.50</strong></td>
      <td style="text-align: right">51.33</td>
      <td style="text-align: right">54.95</td>
      <td style="text-align: right">42.63</td>
    </tr>
    <tr>
      <td style="text-align: left">Fence</td>
      <td style="text-align: right">51.84</td>
      <td style="text-align: right">55.42</td>
      <td style="text-align: right"><strong>55.73</strong></td>
      <td style="text-align: right"><strong>61.48</strong></td>
      <td style="text-align: right">43.25</td>
      <td style="text-align: right">54.71</td>
    </tr>
    <tr>
      <td style="text-align: left">Pole</td>
      <td style="text-align: right">51.49</td>
      <td style="text-align: right">53.41</td>
      <td style="text-align: right"><strong>56.06</strong></td>
      <td style="text-align: right"><strong>58.11</strong></td>
      <td style="text-align: right">39.15</td>
      <td style="text-align: right"><strong>58.37</strong></td>
    </tr>
    <tr>
      <td style="text-align: left">Traffic Light</td>
      <td style="text-align: right">61.06</td>
      <td style="text-align: right">64.20</td>
      <td style="text-align: right"><strong>66.53</strong></td>
      <td style="text-align: right"><strong>67.96</strong></td>
      <td style="text-align: right">47.58</td>
      <td style="text-align: right"><strong>69.71</strong></td>
    </tr>
    <tr>
      <td style="text-align: left">Traffic Sign</td>
      <td style="text-align: right">71.66</td>
      <td style="text-align: right">74.12</td>
      <td style="text-align: right"><strong>75.37</strong></td>
      <td style="text-align: right">75.20</td>
      <td style="text-align: right">61.54</td>
      <td style="text-align: right">68.34</td>
    </tr>
    <tr>
      <td style="text-align: left">Rider</td>
      <td style="text-align: right">37.35</td>
      <td style="text-align: right">51.16</td>
      <td style="text-align: right"><strong>52.51</strong></td>
      <td style="text-align: right"><strong>62.19</strong></td>
      <td style="text-align: right">37.02</td>
      <td style="text-align: right">8.30</td>
    </tr>
    <tr>
      <td style="text-align: left">Motorcycle</td>
      <td style="text-align: right">47.29</td>
      <td style="text-align: right">56.83</td>
      <td style="text-align: right"><strong>61.22</strong></td>
      <td style="text-align: right">59.68</td>
      <td style="text-align: right">27.24</td>
      <td style="text-align: right"><strong>72.11</strong></td>
    </tr>
    <tr>
      <td style="text-align: left">Bicycle</td>
      <td style="text-align: right">69.62</td>
      <td style="text-align: right">72.53</td>
      <td style="text-align: right"><strong>73.46</strong></td>
      <td style="text-align: right"><strong>75.53</strong></td>
      <td style="text-align: right">60.65</td>
      <td style="text-align: right">57.85</td>
    </tr>
  </tbody>
</table>

<p><strong>Table 3.</strong> <em>Per-class IoU comparison across models for thin and boundary-sensitive categories.</em></p>

<p><strong>Analysis.</strong><br />
DDRNet-23 Slim consistently performs well on thin structures due to its architectural emphasis on high-resolution features. However, the SAM3-guided SegFormer closes much of this gap while retaining the flexibility and efficiency of a transformer-based backbone.</p>

<p>Zero-shot SAM3 achieves high IoU on some thin classes (e.g., motorcycle) but performs poorly on semantic consistency, especially for rider and terrain-related categories. This reinforces the conclusion that <strong>SAM3 is best leveraged as a geometry teacher rather than a standalone semantic segmentation model</strong>.</p>

<p>Importantly, large “stuff” classes such as road, building, vegetation, and sky (not shown) remain stable across SegFormer variants, indicating that thin-class-only boundary supervision avoids over-regularization. A regression is observed for the <em>train</em> class in the SAM3-guided model, likely due to class rarity and ambiguous boundaries, and is discussed further in the limitations section.</p>

<p><strong>Summary.</strong><br />
Together, these results demonstrate that distilling geometric knowledge from a foundation model into a lightweight segmentation architecture yields <strong>targeted, interpretable improvements</strong> on known failure modes without increasing inference-time complexity.</p>

<h3 id="82-qualitative-results">8.2 Qualitative Results</h3>

<p>Qualitative comparisons reveal sharper boundaries, improved instance separation, and reduced fragmentation in complex scenes.</p>

<!-- Figure suggestion -->
<!-- Fig. 3: Side-by-side segmentation outputs for baseline vs SAM3-guided model -->

<h2 id="9-analysis-and-observations">9. Analysis and Observations</h2>

<ul>
  <li>Boundary supervision yields targeted improvements without degrading large classes</li>
  <li>Rider and motorcycle classes benefit most</li>
  <li>A regression on the train class is observed and discussed in context of class rarity and boundary ambiguity</li>
</ul>

<h2 id="10-limitations">10. Limitations</h2>

<ul>
  <li>SAM3 introduces additional training-time computational cost</li>
  <li>Boundary supervision does not fully resolve all thin-object errors</li>
  <li>A dedicated thin-object benchmark was proposed but not fully implemented</li>
</ul>

<h2 id="11-conclusion">11. Conclusion</h2>

<p>This project demonstrates that difficulty-aware analysis can reveal systematic weaknesses in semantic segmentation models. By distilling geometric priors from SAM3 into SegFormer through auxiliary boundary supervision, we achieve targeted improvements on thin and boundary-sensitive classes without increasing inference cost.</p>

<h2 id="12-future-work">12. Future Work</h2>

<ul>
  <li>Dedicated thin-object evaluation benchmarks</li>
  <li>Boundary-specific metrics</li>
  <li>Temporal consistency in video segmentation</li>
  <li>Adaptive, class-aware auxiliary supervision</li>
</ul>

<h2 id="references">References</h2>

<p>[1] Cordts, M., et al. <em>The Cityscapes Dataset for Semantic Urban Scene Understanding</em>. CVPR, 2016.<br />
[2] Xie, E., et al. <em>SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</em>. NeurIPS, 2021.<br />
[3] Cheng, B., et al. <em>Masked-attention Mask Transformer for Universal Image Segmentation</em>. CVPR, 2022.<br />
[4] Kirillov, A., et al. <em>Segment Anything</em>. ICCV, 2023.<br />
[5] Shrivastava, A., et al. <em>Training Region-Based Object Detectors with Online Hard Example Mining</em>. CVPR, 2016.</p>]]></content><author><name>Griffin Galimi</name></author><summary type="html"><![CDATA[Semantic segmentation models achieve high overall accuracy on urban datasets, yet systematically fail on thin structures and object boundaries critical for safety-relevant perception. This project presents a diagnostic-driven framework for understanding segmentation failures on Cityscapes and introduces a SAM3-guided boundary supervision method that injects geometric priors into SegFormer. By combining cross-model difficulty analysis with geometry-aware auxiliary training, we demonstrate targeted improvements on thin and boundary-sensitive classes without increasing inference cost.]]></summary></entry><entry><title type="html">Post Template</title><link href="http://localhost:4000/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post.html" rel="alternate" type="text/html" title="Post Template" /><published>2024-01-01T00:00:00-08:00</published><updated>2024-01-01T00:00:00-08:00</updated><id>http://localhost:4000/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post</id><content type="html" xml:base="http://localhost:4000/CS163-Projects-2025Fall/2024/01/01/team00-instruction-to-post.html"><![CDATA[<blockquote>
  <p>This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.</p>
</blockquote>

<!--more-->
<ul id="markdown-toc">
  <li><a href="#main-content" id="markdown-toc-main-content">Main Content</a></li>
  <li><a href="#basic-syntax" id="markdown-toc-basic-syntax">Basic Syntax</a>    <ul>
      <li><a href="#image" id="markdown-toc-image">Image</a></li>
      <li><a href="#table" id="markdown-toc-table">Table</a></li>
      <li><a href="#code-block" id="markdown-toc-code-block">Code Block</a></li>
      <li><a href="#formula" id="markdown-toc-formula">Formula</a></li>
      <li><a href="#more-markdown-syntax" id="markdown-toc-more-markdown-syntax">More Markdown Syntax</a></li>
    </ul>
  </li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="main-content">Main Content</h2>
<p>Your survey starts here. You can refer to the <a href="https://github.com/lilianweng/lil-log/tree/master/_posts">source code</a> of <a href="https://lilianweng.github.io/lil-log/">lil’s blogs</a> for article structure ideas or Markdown syntax. We’ve provided a <a href="https://ucladeepvision.github.io/CS188-Projects-2022Winter/2017/06/21/an-overview-of-deep-learning.html">sample post</a> from Lilian Weng and you can find the source code <a href="https://raw.githubusercontent.com/UCLAdeepvision/CS188-Projects-2022Winter/main/_posts/2017-06-21-an-overview-of-deep-learning.md">here</a></p>

<h2 id="basic-syntax">Basic Syntax</h2>
<h3 id="image">Image</h3>
<p>Please create a folder with the name of your team id under /assets/images/, put all your images into the folder and reference the images in your main content.</p>

<p style="width: 400px; max-width: 100%;">You can add an image to your survey like this:
<img src="/CS163-Projects-2025Fall/assets/images/UCLAdeepvision/object_detection.png" alt="YOLO" /></p>
<p><em>Fig 1. YOLO: An object detection method in computer vision</em> [1].</p>

<p>Please cite the image if it is taken from other people’s work.</p>

<h3 id="table">Table</h3>
<p>Here is an example for creating tables, including alignment syntax.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">column 1</th>
      <th style="text-align: right">column 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">row1</td>
      <td style="text-align: center">Text</td>
      <td style="text-align: right">Text</td>
    </tr>
    <tr>
      <td style="text-align: left">row2</td>
      <td style="text-align: center">Text</td>
      <td style="text-align: right">Text</td>
    </tr>
  </tbody>
</table>

<h3 id="code-block">Code Block</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># This is a sample code block
import torch
print (torch.__version__)
</code></pre></div></div>

<h3 id="formula">Formula</h3>
<p>Please use latex to generate formulas, such as:</p>

\[\tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i + (1-\alpha) \mathbf{z}_i}{1-\alpha^t}\]

<p>or you can write in-text formula \(y = wx + b\).</p>

<h3 id="more-markdown-syntax">More Markdown Syntax</h3>
<p>You can find more Markdown syntax at <a href="https://www.markdownguide.org/basic-syntax/">this page</a>.</p>

<h2 id="reference">Reference</h2>
<p>Please make sure to cite properly in your work, for example:</p>

<p>[1] Redmon, Joseph, et al. “You only look once: Unified, real-time object detection.” <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2016.</p>

<hr />]]></content><author><name>UCLAdeepvision</name></author><summary type="html"><![CDATA[This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.]]></summary></entry></feed>